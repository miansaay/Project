{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFG de Miguel Ángel\n",
    "\n",
    "En este documento vamos a ir haciendo un pequeño análisis del TFG convenientemente documentado, que servirá en el futuro para poder realizar la memoria y el análisis completo.\n",
    "\n",
    "## Datos\n",
    "\n",
    "A continuación un sucinto resumen de cómo es la estructura de los datos\n",
    "\n",
    "### ECGs\n",
    "\n",
    "Los ECGs son de una única derivación (un canal), con una duración entre **9 segs a 60 segs**. \n",
    "\n",
    "* Una reflexión, para el esquema de clasificación sin utilizar las redes neuronales recurrentes, podríamos hacer que todas las series tuviesen la misma longitud, por ejemplo 60 seg, de tal forma que las más cortas se podrían generar replicando de alguna forma la serie para que tuviese 60 seg.\n",
    "\n",
    "* La **frecuencia de muestreo** es de 300 Hz\n",
    "\n",
    "* Los tipos de posibles señales son:\n",
    "    1. Normal - N\n",
    "    2. AF - A\n",
    "    3. Otros ritmos - O\n",
    "    4. Ruidoso - ~\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "La base de datos de training tiene la información de la clasificación del ritmo en los csv que están en la carpeta training. **Es necesario leer la cabecera, pero no continene la información de la clasificación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a leer un par de ejemplo, así como la cabecera\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tfg_tools import read_challenge_mat_files, processing_ecg, plot_all_records\n",
    "\n",
    "ecg, header = read_challenge_mat_files('A00001.mat','./')\n",
    "fs = 250 #float(header['fs'])\n",
    "t = np.arange(0,len(ecg))/fs \n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(t,ecg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_filtered = processing_ecg(ecg)\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(t,ecg,label='original')\n",
    "plt.plot(t,ecg_filtered,label='filtered 2-45 Hz')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zoom\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(t[t<10],ecg[t<10],label='original')\n",
    "plt.plot(t[t<10],ecg_filtered[t<10],label='filtered')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter Preprocesamiento y análisis exploratorio de señales\n",
    "\n",
    "**TO DO**\n",
    "\n",
    "* Crear una rutina que te pinte todas las señales. Un for que vaya pintando todas las señales y que se en la figura pinte información de los ejes (segundos y mv) y en la leyenda indique cómo ha sido clasificada la señal. Convendría pintar la señal original y la filtrada entre 2 y 45 Hz. Que no pase a la siguiente iteración del bucle hasta que, por ejemplo, se pulse un click de raton o alguna tecla.\n",
    "\n",
    "* Sacar la siguiente información:\n",
    "    * Número de sujetos en cada clase (esto lo puedes sacar del fichero en el que viene la clasificación de cada señal)\n",
    "    * Longitudes de las señales: Una función que se cree un vector con todas las longitudes de todas las señales, y que pinte un histograma de las longitudes.\n",
    "   \n",
    "* Con esto podemos divisar los modeos de deep learning para clasificación\n",
    "\n",
    "\n",
    "# NOTA:\n",
    "\n",
    "<font color='red'>Miguel Ángel, lo que te pedía es que tu hicieses el análisis y completases este notebook con esa información, porque todo lo que escribas aquí, también se puede utilizar luego para la memoria</font>\n",
    "\n",
    "\n",
    "### Número de sujetos en cada clase:\n",
    "\n",
    "A continuación obtenemos el resultado de sujetos por cada clase:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the number of patietns in each class\n",
    "\n",
    "from tfg_tools import get_distribution_classes\n",
    "\n",
    "class_dist = get_distribution_classes()\n",
    "\n",
    "print(class_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar existe un claro desbalanceo: El caso de los registros normales es siete veces mayor que el caso de las señales con AF. Habrá que escoger alguna técnica de balanceado. Lo primero que recomendamos es echar un vistazo al siguiente post:\n",
    "\n",
    "[Imbalanced classes](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/).\n",
    "\n",
    "De las técnicas que tenemos en este caso, yo propondría varias, teniendo en cuenta que esto es un TFG:\n",
    "\n",
    "1. Entrenar para mejorar otra métrica que accuracy, por ejemplo $\\kappa$ o $F_1 score$.\n",
    "2. Random over-sampling under-sampling: podemos utilizar el siguiente módulo (imbalanced-learning)(http://contrib.scikit-learn.org/imbalanced-learn/stable/index.html)\n",
    "3. Utilizar costes diferentes dependiendo de la misclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitudes de las señales\n",
    "\n",
    "Un problema cuando trabajamos con modelos que no tengan en cuenta la parte dinámica es la diferente longitud de las señales. Vamos a obtener un histograma de las diferentes longitudes de señales, para tener una idea del rango de posibles longitudes, para proponer una estrategia cuando implementemos el modelo de deep learning sin memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfg_tools import get_distribution_length\n",
    "\n",
    "lengths = get_distribution_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Número de señales con menos de 9000\n",
    "\n",
    "print('El numero de senales con menos de 9000 es: ',np.sum(np.array(lengths)<9000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO\n",
    "\n",
    "Vamos a considerar **longitud de señales = 9000**, todas las que estén por debajo (967) no van a entrar en el conjunto, al menos en primera instancia. Necesitamos verificar de qué clase son las señales con menos de 9000. \n",
    "\n",
    "Tu próxima tarea es verificar las distribución por clases de esas 967 señales con menos de 9000. Para las que tienen más de 9000 muestras vamos a utilizar sólo las primeras 9000.\n",
    "\n",
    "El problema lo resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfg_tools import get_distribution_less9000\n",
    "\n",
    "classes_less_9000 = get_distribution_less9000()\n",
    "\n",
    "print(classes_less_9000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos apreciar en los resultados de las señales con longitud menor de 9000, las señales normales son las más abundantes en comparanción con el resto. \n",
    "\n",
    "**Cuidado con estas afirmaciones Miguel Ángel** lo que dices es cierto, pero en valor absoluto, el problema es que no hay la misma cantidad inicial de datos, de hecho: AF 113/783 = 0.19 approx 20%, mientras que Normal 521/5050 = 0.10 approx 10%\n",
    "\n",
    "De esta manera podemose decir que, para el caso de una NN sin memoria, con señales de entrada de longitud 9000 muestras, vamos a tener la siguiente distribución:\n",
    "\n",
    "* **AF** = 738 - 113 = **625**\n",
    "* **Noisy** = 284 - 139 = **145**\n",
    "* **Other** = 2456 - 194 = **2262**\n",
    "* **Normal** = 5050 - 521 = **4529**\n",
    "\n",
    "\n",
    "## TO_DO\n",
    "\n",
    "Los siguientes pasos serían:\n",
    "\n",
    "1. Partir convenientemente los datos en training y test, yo utilizaría alguna herramienta de sklearn para mantener las proporciones (de las clases) en test, hay herramientas como stratified.\n",
    "2. Elegir alguna estrategia para comenzar con el tratamiento del desbalanceo: por ejemplo: under-sampling, over-sampling (aquí, vamos a utilizar el modulo que puse anteriormente, parecido a sklearn), y métrica.\n",
    "3. Constuir un esquema sencillo de red neuronal, y luego complicarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of stratified split into train and test \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "x = np.ones(100)\n",
    "y = [0,0,0,0,1,1,1,1,1,1] *10 #proportion of labels 6 out of 10 are class 1\n",
    "y = np.array(y)\n",
    "\n",
    "#split with previous shuffle. Test size = 33%\n",
    "\n",
    "skf = StratifiedShuffleSplit(n_splits = 1,test_size = 0.33)\n",
    "\n",
    "for train, test in skf.split(x,y):\n",
    "    print(\"%s %s\" % (train,test))\n",
    "\n",
    "#let's check proportions\n",
    "\n",
    "print(\"Proportion of class 1 in original dataset %.2f %%: \" % (np.sum(y)/float(len(y))))\n",
    "print(\"Proportion of class 1 in training dataset %.2f %%: \" % (np.sum(y[train])/float(len(y[train]))))\n",
    "print(\"Proportion of class 1 in test dataset %.2f %%: \" % (np.sum(y[test])/float(len(y[test]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter : Modelos de redes neuronales\n",
    "\n",
    "Vamos a comenzar utilizando una red neuronal ..\n",
    "\n",
    "$$y(t) = \\frac{1}{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Creación Matrix X y vector y\n",
    "   \n",
    "   En esta sección vamos a crear una matrix X con todos los casos con más de 9000 muestras. Pues vamos a elegir trabajar con señales de 9000 muestras. Una vez creadas estas matrices y vectores, los vamos a salvar en disco para trabajar más rápido con ellas. Esta va a ser nuestro conjunto de inicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfg_tools import filter_data\n",
    "\n",
    "X,y = filter_data()\n",
    "\n",
    "#convert to a code y\n",
    "y_new = codify_y(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save with numpy x and y\n",
    "np.save('X_mat',X)\n",
    "\n",
    "np.save('y',y_new)\n",
    "\n",
    "np.save('y_str',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sección: Separación en Training y Test\n",
    "\n",
    "**Miguel Ángel, comenta aquí lo que vas a hacer para realizar la separación. En las celdas siguientes crea el código para realizarlo**\n",
    "\n",
    "Dividimos los datos en training y test. Sólo se han considerado las señales de longitud 9000. Esta información se ha dividido en 80% para training y el 20% para test. Para la separación se ha utilizado la función StratifiedShuffleSplit. A continuación se muestran los índices del conjunto de entrenamiento, seguidos de los índices del conjunto de prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tfg_tools import filter_data\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "#np.set_printoptions(threshold='nan')\n",
    "\n",
    "#cargamos os datos\n",
    "X = np.load('X_mat.npy')\n",
    "y = np.load('y.npy')\n",
    "\n",
    "print(X[0,0])\n",
    "#X = X.astype('float32',copy = False)\n",
    "#y = y.astype('float32',copy = False)\n",
    "\n",
    "skf = StratifiedShuffleSplit(n_splits = 1,test_size = 0.20)\n",
    "\n",
    "for train, test in skf.split(X,y):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "\n",
    "np.save('X_train',X_train)\n",
    "np.save('X_test',X_test)\n",
    "np.save('y_train',y_train)\n",
    "np.save('y_test',y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from tfg_tools import count,filter_data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X,y = filter_data()  \n",
    "#X_res, y_res = SMOTE(kind='svm').fit_sample(X, y)\n",
    "X_res, y_res = SMOTE().fit_sample(X, y)\n",
    "print count(y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para equilibrar las clases hemos utilizado la técnica de sobremuestreo minoritario sintético (SMOTE). Esta técnica consiste en generar nuevas muestras por interpolación. Como podemos apreciar ahora todas las clases tiene el mismo número de muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
